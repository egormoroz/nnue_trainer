so far I use lamda = 1 everywhere. I doesn't seem to improve the net (TODO: check this).
UPD: result interpolation has been bugged all this time, that's why lambda < 1 didn't work:
the result was ALWAYS abosolute, whereas in the loss calculation is is assumed to be relative to the pov (1: stm wins, 0: stm loses).
UPD2: Nope, not only does it train terribly, but is also is noticable worse.

Training from scratch rather than starting from the previous best net is worse, NOTED.

currently REALLY SUCKS against FAST engines @ fast time controls??
each iteration is +50-100 elo basically
current estimated elo is 2900+ (it plays well (wins) against Zangdar 2.24, which is 2971 elo, any many engines in the 2800-2900 range), so simply switching from PSQT eval to shitty NNUE gained 200+ elo lmao. (without any other significant changes). It bites the dust against seawall @ 10+0.2 though...

0_d7.nnue:

The first net that actually beat baseline by a lot (~50 elo or more?..). It was trained on the first dataset, where selfplay made N random moves, chosen as a random pv from the multipv low depth search (multipv=3, depth=2). The result is the startign position for further selfplay. 75mil positions (>= 50mil unique), mindepth 7, mpv 3, maxtime 5. (4gb TT).

1_d7.nnue

Same selfplay settings, but I generated ~40mil more positions and retrained from scratch. A shitton of elo gained, compared to previous net. There are two varants: net_49 and net_70. net_49 is consistently better than previous version. net_70 is weird, but sometimes is better. Nevermind, 70 is outright better....

2_net_99.nnue

1_d7, finetuned on 50mil d6 NNUE(0_d7.nnue) positions for 100 epochs. Proved to be much stronger.

3_net_70.nnue

2_net_70.nnue, resumed training on like 40mil data generated using 2_net_99.nnue. epoch 70 seems better than epoch 99. Is 40mil data too little and we have overfit??..

4_x2_net_60.nnue

A modest improvement. It wasn't actually x2 data, m.b. about 1.4 more.

5_net_190.nnue

Another modest improvement. Trained on 100mil positions generated by 4_x2_net_60.nnue. The dataset is d6nnv3.
** IMPORTANT: Here I fixed a bug where there was a AVX2 register spill in the biggest fucking linear layer. Fixing it game x1.5-2.0 NPS.
We are okay against princhess now, but I couldn't get it to set the size of the hash table, so the test was 64MB hash vs whatever default it had (I think 16MB).
Also, this version OBLITERATES Zangdar 2.24 (2972 elo) @ 10+0.2. Additionally, the dataset seemed to be quite good: 90% unique positions, the histogram and WDL sigmoid looked good, and there didn't seemed to be no overfitting whatsoever: the more epochs the better. However, the training was very slow: it took ~200 epochs to reach the 0.002011 mean epoch loss.
Also I observe that the min and max eval values are increasing: the first versions were typically giving evals e: |e| < 1000. I have a test position with 2 queens advantage, and witch each iteration (of nets, given in this file) the evaluation value increases. Also, the selfplay speed up by like 15% for the same settings (d6, pv3, ld50, 16 threads). Maybe the simd fix is the reason... (not sure though, the server has AVX512, and the engine is compiled with -march=native), so there is a gazilion registers.

6_net_120.nnue

Trained on d6nnv4 (150mil). A mild improvement.